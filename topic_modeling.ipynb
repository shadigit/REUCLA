{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '../../base_codes')\n",
    "sys.path.insert(0, '../../data_specific_codes')\n",
    "sys.path.insert(0, '../../utility_codes')\n",
    "\n",
    "from RE_init import *\n",
    "from main_functions import *\n",
    "from utility_functions import *\n",
    "from SDDb_utility_functions import *\n",
    "\n",
    "#%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "based_dir = ''\n",
    "file_input_postfix = \".csv\"\n",
    "input_name_prefix = 'SDDb_all_clean'#'SDDb_car_clean'#_relations_-1'#_filter(car)'\n",
    "file_input_name = input_name_prefix + file_input_postfix\n",
    "\n",
    "\n",
    "\n",
    "DATA_SET = 'sddb-car'\n",
    "#output_dir = \"\"\n",
    "#based_dir = based_dir + output_dir\n",
    "\n",
    "def read_df_rel(based_dir, file_input_name):\n",
    "    file_input = based_dir + file_input_name    \n",
    "    ff = open(file_input)\n",
    "    delim=\",\"\n",
    "    df = pd.read_csv(file_input,delimiter=delim,header=0)        \n",
    "    return df\n",
    "\n",
    "df_rels = read_df_rel(based_dir, file_input_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33453\n",
      "33448\n"
     ]
    }
   ],
   "source": [
    "print len(df_rels)\n",
    "df_rels.dropna(subset=[\"text\"], how=\"all\", inplace=True)\n",
    "print len(df_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_rels[\"person id\"].unique()\n",
    "b = a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12560"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [i.split(\":\")[0] for i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_unique = set(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ad2007_lola',\n",
       " 'ad2007_paul',\n",
       " 'ad2007_sophia',\n",
       " 'ad2007_will',\n",
       " 'african_church_dreams1',\n",
       " 'african_church_dreams2',\n",
       " 'african_church_dreams3',\n",
       " 'anna_kingsford_series',\n",
       " 'anthropological_dreams',\n",
       " 'bea_series',\n",
       " 'betty_series',\n",
       " 'beverly_1986',\n",
       " 'beverly_1996',\n",
       " 'beverly_2006',\n",
       " 'beverly_2016',\n",
       " 'brianna1',\n",
       " 'brianna2',\n",
       " 'brianna3',\n",
       " 'brianna_journal',\n",
       " 'bs_250',\n",
       " 'bt_2013',\n",
       " 'carl_series',\n",
       " 'doug_series',\n",
       " 'dreambank_barack',\n",
       " 'dreambank_hilary',\n",
       " 'dreamboarda',\n",
       " 'dreamsfield',\n",
       " 'ella_series',\n",
       " 'film_dreams',\n",
       " 'gackenbach',\n",
       " 'gackenbach_nightmares_2010_107',\n",
       " 'gackenbach_nightmares_2010_116',\n",
       " 'gackenbach_nightmares_2010_124',\n",
       " 'gackenbach_nightmares_2010_13',\n",
       " 'gackenbach_nightmares_2010_131',\n",
       " 'gackenbach_nightmares_2010_17',\n",
       " 'gackenbach_nightmares_2010_2',\n",
       " 'gackenbach_nightmares_2010_22',\n",
       " 'gackenbach_nightmares_2010_26',\n",
       " 'gackenbach_nightmares_2010_3',\n",
       " 'gackenbach_nightmares_2010_37',\n",
       " 'gackenbach_nightmares_2010_44',\n",
       " 'gackenbach_nightmares_2010_49',\n",
       " 'gackenbach_nightmares_2010_50',\n",
       " 'gackenbach_nightmares_2010_53',\n",
       " 'gackenbach_nightmares_2010_6',\n",
       " 'gackenbach_nightmares_2010_66',\n",
       " 'gackenbach_nightmares_2010_70',\n",
       " 'gackenbach_nightmares_2010_72',\n",
       " 'gackenbach_nightmares_2010_78',\n",
       " 'gackenbach_nightmares_2010_79',\n",
       " 'gackenbach_nightmares_2010_96',\n",
       " 'gackenbach_nightmares_2010_98',\n",
       " 'harris_2011',\n",
       " 'harris_2012',\n",
       " 'harris_2013s',\n",
       " 'harris_2013w',\n",
       " 'harris_youth_2011',\n",
       " 'historical_dreams',\n",
       " 'hvdc_f0',\n",
       " 'hvdc_f1',\n",
       " 'hvdc_f2',\n",
       " 'hvdc_f3',\n",
       " 'hvdc_f4',\n",
       " 'hvdc_m0',\n",
       " 'hvdc_m1',\n",
       " 'hvdc_m2',\n",
       " 'hvdc_m3',\n",
       " 'hvdc_m4',\n",
       " 'janet1',\n",
       " 'jasmine_journala',\n",
       " 'jasmine_journalb',\n",
       " 'jasmine_journalc',\n",
       " 'jasmine_journald',\n",
       " 'jordan_series',\n",
       " 'kahan_sleep_wake_1013',\n",
       " 'kahan_sleep_wake_1064',\n",
       " 'kahan_sleep_wake_114',\n",
       " 'kahan_sleep_wake_1176',\n",
       " 'kahan_sleep_wake_1197',\n",
       " 'kahan_sleep_wake_1199',\n",
       " 'kahan_sleep_wake_1202',\n",
       " 'kahan_sleep_wake_1242',\n",
       " 'kahan_sleep_wake_1299',\n",
       " 'kahan_sleep_wake_1313',\n",
       " 'kahan_sleep_wake_1387',\n",
       " 'kahan_sleep_wake_1399',\n",
       " 'kahan_sleep_wake_1476',\n",
       " 'kahan_sleep_wake_1487',\n",
       " 'kahan_sleep_wake_1494',\n",
       " 'kahan_sleep_wake_1495',\n",
       " 'kahan_sleep_wake_1519',\n",
       " 'kahan_sleep_wake_1563',\n",
       " 'kahan_sleep_wake_1577',\n",
       " 'kahan_sleep_wake_160',\n",
       " 'kahan_sleep_wake_1626',\n",
       " 'kahan_sleep_wake_1776',\n",
       " 'kahan_sleep_wake_1779',\n",
       " 'kahan_sleep_wake_1806',\n",
       " 'kahan_sleep_wake_1812',\n",
       " 'kahan_sleep_wake_1874',\n",
       " 'kahan_sleep_wake_1880',\n",
       " 'kahan_sleep_wake_1915',\n",
       " 'kahan_sleep_wake_2006',\n",
       " 'kahan_sleep_wake_208',\n",
       " 'kahan_sleep_wake_2320',\n",
       " 'kahan_sleep_wake_2373',\n",
       " 'kahan_sleep_wake_2409',\n",
       " 'kahan_sleep_wake_2415',\n",
       " 'kahan_sleep_wake_2425',\n",
       " 'kahan_sleep_wake_2443',\n",
       " 'kahan_sleep_wake_247',\n",
       " 'kahan_sleep_wake_2541',\n",
       " 'kahan_sleep_wake_256',\n",
       " 'kahan_sleep_wake_2586',\n",
       " 'kahan_sleep_wake_264',\n",
       " 'kahan_sleep_wake_2675',\n",
       " 'kahan_sleep_wake_2790',\n",
       " 'kahan_sleep_wake_2882',\n",
       " 'kahan_sleep_wake_2883',\n",
       " 'kahan_sleep_wake_2909',\n",
       " 'kahan_sleep_wake_2929',\n",
       " 'kahan_sleep_wake_3009',\n",
       " 'kahan_sleep_wake_301',\n",
       " 'kahan_sleep_wake_3042',\n",
       " 'kahan_sleep_wake_3049',\n",
       " 'kahan_sleep_wake_3059',\n",
       " 'kahan_sleep_wake_308',\n",
       " 'kahan_sleep_wake_3098',\n",
       " 'kahan_sleep_wake_31',\n",
       " 'kahan_sleep_wake_314',\n",
       " 'kahan_sleep_wake_3192',\n",
       " 'kahan_sleep_wake_3232',\n",
       " 'kahan_sleep_wake_330',\n",
       " 'kahan_sleep_wake_3359',\n",
       " 'kahan_sleep_wake_338',\n",
       " 'kahan_sleep_wake_3396',\n",
       " 'kahan_sleep_wake_3408',\n",
       " 'kahan_sleep_wake_3433',\n",
       " 'kahan_sleep_wake_3481',\n",
       " 'kahan_sleep_wake_3482',\n",
       " 'kahan_sleep_wake_3565',\n",
       " 'kahan_sleep_wake_3567',\n",
       " 'kahan_sleep_wake_3663',\n",
       " 'kahan_sleep_wake_3664',\n",
       " 'kahan_sleep_wake_3772',\n",
       " 'kahan_sleep_wake_3773',\n",
       " 'kahan_sleep_wake_3810',\n",
       " 'kahan_sleep_wake_3817',\n",
       " 'kahan_sleep_wake_3838',\n",
       " 'kahan_sleep_wake_3891',\n",
       " 'kahan_sleep_wake_3963',\n",
       " 'kahan_sleep_wake_398',\n",
       " 'kahan_sleep_wake_4040',\n",
       " 'kahan_sleep_wake_4058',\n",
       " 'kahan_sleep_wake_4082',\n",
       " 'kahan_sleep_wake_4141',\n",
       " 'kahan_sleep_wake_4270',\n",
       " 'kahan_sleep_wake_4317',\n",
       " 'kahan_sleep_wake_4325',\n",
       " 'kahan_sleep_wake_4413',\n",
       " 'kahan_sleep_wake_4445',\n",
       " 'kahan_sleep_wake_4525',\n",
       " 'kahan_sleep_wake_4533',\n",
       " 'kahan_sleep_wake_4546',\n",
       " 'kahan_sleep_wake_461',\n",
       " 'kahan_sleep_wake_4615',\n",
       " 'kahan_sleep_wake_4687',\n",
       " 'kahan_sleep_wake_4690',\n",
       " 'kahan_sleep_wake_4711',\n",
       " 'kahan_sleep_wake_4739',\n",
       " 'kahan_sleep_wake_4863',\n",
       " 'kahan_sleep_wake_4916',\n",
       " 'kahan_sleep_wake_4940',\n",
       " 'kahan_sleep_wake_4949',\n",
       " 'kahan_sleep_wake_4961',\n",
       " 'kahan_sleep_wake_4994',\n",
       " 'kahan_sleep_wake_5050',\n",
       " 'kahan_sleep_wake_5092',\n",
       " 'kahan_sleep_wake_5132',\n",
       " 'kahan_sleep_wake_5189',\n",
       " 'kahan_sleep_wake_5212',\n",
       " 'kahan_sleep_wake_5268',\n",
       " 'kahan_sleep_wake_5310',\n",
       " 'kahan_sleep_wake_5454',\n",
       " 'kahan_sleep_wake_5548',\n",
       " 'kahan_sleep_wake_5700',\n",
       " 'kahan_sleep_wake_5703',\n",
       " 'kahan_sleep_wake_5770',\n",
       " 'kahan_sleep_wake_5794',\n",
       " 'kahan_sleep_wake_582',\n",
       " 'kahan_sleep_wake_5912',\n",
       " 'kahan_sleep_wake_5913',\n",
       " 'kahan_sleep_wake_5947',\n",
       " 'kahan_sleep_wake_6037',\n",
       " 'kahan_sleep_wake_6060',\n",
       " 'kahan_sleep_wake_6199',\n",
       " 'kahan_sleep_wake_6249',\n",
       " 'kahan_sleep_wake_6262',\n",
       " 'kahan_sleep_wake_6272',\n",
       " 'kahan_sleep_wake_6340',\n",
       " 'kahan_sleep_wake_6426',\n",
       " 'kahan_sleep_wake_654',\n",
       " 'kahan_sleep_wake_6819',\n",
       " 'kahan_sleep_wake_6863',\n",
       " 'kahan_sleep_wake_6868',\n",
       " 'kahan_sleep_wake_7184',\n",
       " 'kahan_sleep_wake_7211',\n",
       " 'kahan_sleep_wake_726',\n",
       " 'kahan_sleep_wake_7400',\n",
       " 'kahan_sleep_wake_7422',\n",
       " 'kahan_sleep_wake_7519',\n",
       " 'kahan_sleep_wake_7694',\n",
       " 'kahan_sleep_wake_7777',\n",
       " 'kahan_sleep_wake_7828',\n",
       " 'kahan_sleep_wake_7878',\n",
       " 'kahan_sleep_wake_7903',\n",
       " 'kahan_sleep_wake_801',\n",
       " 'kahan_sleep_wake_8030',\n",
       " 'kahan_sleep_wake_8033',\n",
       " 'kahan_sleep_wake_804',\n",
       " 'kahan_sleep_wake_8041',\n",
       " 'kahan_sleep_wake_8063',\n",
       " 'kahan_sleep_wake_8080',\n",
       " 'kahan_sleep_wake_8118',\n",
       " 'kahan_sleep_wake_8192',\n",
       " 'kahan_sleep_wake_824',\n",
       " 'kahan_sleep_wake_8252',\n",
       " 'kahan_sleep_wake_8282',\n",
       " 'kahan_sleep_wake_8392',\n",
       " 'kahan_sleep_wake_8418',\n",
       " 'kahan_sleep_wake_8419',\n",
       " 'kahan_sleep_wake_8422',\n",
       " 'kahan_sleep_wake_8505',\n",
       " 'kahan_sleep_wake_8537',\n",
       " 'kahan_sleep_wake_8717',\n",
       " 'kahan_sleep_wake_8771',\n",
       " 'kahan_sleep_wake_8787',\n",
       " 'kahan_sleep_wake_8841',\n",
       " 'kahan_sleep_wake_893',\n",
       " 'kahan_sleep_wake_909',\n",
       " 'kahan_sleep_wake_9161',\n",
       " 'kahan_sleep_wake_9191',\n",
       " 'kahan_sleep_wake_9252',\n",
       " 'kahan_sleep_wake_9285',\n",
       " 'kahan_sleep_wake_9306',\n",
       " 'kahan_sleep_wake_9403',\n",
       " 'kahan_sleep_wake_9569',\n",
       " 'kahan_sleep_wake_9575',\n",
       " 'kahan_sleep_wake_9666',\n",
       " 'kahan_sleep_wake_9691',\n",
       " 'kahan_sleep_wake_9797',\n",
       " 'kahan_sleep_wake_9801',\n",
       " 'kahan_sleep_wake_9829',\n",
       " 'kahan_sleep_wake_9889',\n",
       " 'kahan_sleep_wake_9903',\n",
       " 'kahan_sleep_wake_9932',\n",
       " 'kahan_sleep_wake_998',\n",
       " 'kb_dj_2010',\n",
       " 'kb_dj_2011',\n",
       " 'kb_dj_2012a',\n",
       " 'kb_dj_2013',\n",
       " 'kb_dj_2014',\n",
       " 'kb_dj_2015',\n",
       " 'kb_dj_2016',\n",
       " 'kb_dj_2016a',\n",
       " 'kb_dj_2017',\n",
       " 'km2015',\n",
       " 'krippner_survey1',\n",
       " 'lawrence_journal_2011',\n",
       " 'literary_dreams',\n",
       " 'lola_series',\n",
       " 'lucrecia_1',\n",
       " 'magnolia1',\n",
       " 'mehinaku',\n",
       " 'miami_home',\n",
       " 'miami_lab',\n",
       " 'mike_journal',\n",
       " 'mrds',\n",
       " 'nadine_series',\n",
       " 'nan_series',\n",
       " 'natural_scientist_journal',\n",
       " 'nepaldreams',\n",
       " 'paulidreams',\n",
       " 'rb_dreams',\n",
       " 'recent_dreams_2015',\n",
       " 'sears_2014',\n",
       " 'sersa',\n",
       " 'sersa2_2012',\n",
       " 'sheila_series',\n",
       " 'smith_2012',\n",
       " 'stuart_series',\n",
       " 'swedenborg_dreams',\n",
       " 'tad_series',\n",
       " 'tanya1',\n",
       " 'td2017',\n",
       " 'valli2015a',\n",
       " 'valli2015b',\n",
       " 'van_series',\n",
       " 'zogby_052110'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text, Remove stop words, Tokenize text, Apply LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named stop_words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5e476a0a82e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named stop_words"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "\n",
    "\n",
    "doc_set = df_rels[\"text\"]\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "start_time = time.time()\n",
    "nltk_errors_cnt = 0\n",
    "# loop through document list\n",
    "for ind,d in enumerate(doc_set):\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    #raw = raw.translate(None, string.punctuation)\n",
    "    raw = d.lower()\n",
    "    \n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    try:\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    except:\n",
    "        print \"doc\", d\n",
    "        print \"stopped tokens: \", stopped_tokens\n",
    "        nltk_errors_cnt += 1\n",
    "        continue\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print \"Execution time: \", (time.time()-start_time)/60.0, \" minutes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time:  67.319705534  minutes.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=50, id2word = dictionary, passes=20)\n",
    "\n",
    "print \"Execution time: \", (time.time()-start_time)/60.0, \" minutes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look into topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "0.057*like + 0.048*someth + 0.029*think + 0.028*s + 0.015*also + 0.015*look + 0.014*one + 0.014*kind\n",
      "Topic  1\n",
      "0.040*photo + 0.029*french + 0.024*puppi + 0.024*greet + 0.021*weight + 0.021*tan + 0.019*50 + 0.018*pound\n",
      "Topic  2\n",
      "0.194*dream + 0.080*rememb + 0.021*time + 0.019*woke + 0.018*just + 0.015*last + 0.014*night + 0.014*realli\n",
      "Topic  3\n",
      "0.050*heart + 0.045*problem + 0.042*zombi + 0.039*film + 0.032*toilet + 0.023*castl + 0.021*tube + 0.019*medic\n",
      "Topic  4\n",
      "0.058*card + 0.043*island + 0.028*trump + 0.025*interview + 0.024*recal + 0.022*villag + 0.021*ladder + 0.019*jack\n",
      "Topic  5\n",
      "0.087*fire + 0.025*burn + 0.022*unknown + 0.021*worker + 0.017*tornado + 0.015*hate + 0.014*world + 0.014*none\n",
      "Topic  6\n",
      "0.048*dont + 0.043*im + 0.020*didnt + 0.020*cant + 0.019*goodby + 0.011*that + 0.008*id + 0.008*bull\n",
      "Topic  7\n",
      "0.120*friend + 0.051*girl + 0.026*talk + 0.023*boy + 0.022*boyfriend + 0.018*school + 0.017*us + 0.017*one\n",
      "Topic  8\n",
      "0.031*john + 0.025*fellow + 0.024*desert + 0.023*sun + 0.018*enjoy + 0.018*destroy + 0.017*shine + 0.016*breast\n",
      "Topic  9\n",
      "0.087*money + 0.039*buy + 0.035*memor + 0.034*pay + 0.030*bill + 0.025*king + 0.023*sell + 0.020*bought\n",
      "Topic  10\n",
      "0.135*car + 0.063*drive + 0.033*road + 0.033*park + 0.026*street + 0.024*go + 0.023*get + 0.017*back\n",
      "Topic  11\n",
      "0.095*hous + 0.041*s + 0.035*mom + 0.032*room + 0.028*like + 0.026*dad + 0.021*live + 0.020*home\n",
      "Topic  12\n",
      "0.064*water + 0.024*fall + 0.020*swim + 0.016*pool + 0.016*boat + 0.013*mountain + 0.012*ground + 0.012*jump\n",
      "Topic  13\n",
      "0.122*sister + 0.082*brother + 0.071*bu + 0.029*mall + 0.026*cousin + 0.025*grandma + 0.016*search + 0.014*younger\n",
      "Topic  14\n",
      "0.059*white + 0.057*dress + 0.045*wear + 0.038*black + 0.035*color + 0.033*blue + 0.031*look + 0.030*man\n",
      "Topic  15\n",
      "0.041*like + 0.030*one + 0.017*peopl + 0.017*look + 0.017*guy + 0.016*around + 0.014*point + 0.014*anoth\n",
      "Topic  16\n",
      "0.104*bathroom + 0.057*apart + 0.043*bike + 0.042*shower + 0.040*field + 0.027*club + 0.021*golf + 0.020*use\n",
      "Topic  17\n",
      "0.049*tabl + 0.048*eat + 0.047*food + 0.027*restaur + 0.025*drink + 0.022*dinner + 0.021*suit + 0.020*sit\n",
      "Topic  18\n",
      "0.102*tree + 0.040*elev + 0.036*fenc + 0.026*lion + 0.018*swing + 0.017*favorit + 0.015*wine + 0.013*juli\n",
      "Topic  19\n",
      "0.074*hair + 0.072*woman + 0.049*girlfriend + 0.045*sex + 0.037*kiss + 0.026*sexual + 0.023*girl + 0.023*date\n",
      "Topic  20\n",
      "0.098*hotel + 0.036*shift + 0.024*jani + 0.019*unit + 0.019*chain + 0.017*gang + 0.017*statu + 0.016*lobbi\n",
      "Topic  21\n",
      "0.118*peopl + 0.079*group + 0.030*women + 0.027*men + 0.025*mani + 0.020*one + 0.019*larg + 0.014*us\n",
      "Topic  22\n",
      "0.102*train + 0.055*snake + 0.038*track + 0.034*teeth + 0.027*station + 0.023*mouth + 0.017*candi + 0.017*bite\n",
      "Topic  23\n",
      "0.059*bag + 0.053*ship + 0.053*ring + 0.025*object + 0.019*duck + 0.018*tom + 0.017*court + 0.015*frame\n",
      "Topic  24\n",
      "0.116*movi + 0.073*beach + 0.064*marri + 0.047*wed + 0.029*sand + 0.024*ghost + 0.023*watch + 0.020*theater\n",
      "Topic  25\n",
      "0.155*work + 0.050*book + 0.037*job + 0.033*read + 0.023*write + 0.017*paper + 0.015*comput + 0.012*letter\n",
      "Topic  26\n",
      "0.042*hors + 0.038*box + 0.038*bear + 0.029*ice + 0.017*13 + 0.017*hunt + 0.017*wood + 0.017*ride\n",
      "Topic  27\n",
      "0.050*go + 0.050*school + 0.025*get + 0.024*colleg + 0.024*high + 0.023*student + 0.023*take + 0.021*meet\n",
      "Topic  28\n",
      "0.035*feel + 0.017*time + 0.017*will + 0.012*love + 0.011*even + 0.011*way + 0.010*seem + 0.010*life\n",
      "Topic  29\n",
      "0.038*stone + 0.038*giant + 0.035*star + 0.031*plant + 0.030*member + 0.025*ski + 0.023*cream + 0.019*design\n",
      "Topic  30\n",
      "0.080*danc + 0.045*music + 0.043*play + 0.042*sing + 0.042*song + 0.037*stage + 0.034*mr + 0.028*perform\n",
      "Topic  31\n",
      "0.057*man + 0.051*kill + 0.030*shot + 0.029*gun + 0.028*shoot + 0.021*fight + 0.018*attack + 0.017*bodi\n",
      "Topic  32\n",
      "0.027*room + 0.026*walk + 0.026*door + 0.022*look + 0.020*hous + 0.016*see + 0.015*open + 0.015*one\n",
      "Topic  33\n",
      "0.031*cat + 0.026*fish + 0.019*clean + 0.019*hous + 0.018*lot + 0.018*peopl + 0.016*big + 0.014*go\n",
      "Topic  34\n",
      "0.057*s + 0.053*m + 0.040*go + 0.030*say + 0.030*get + 0.019*see + 0.018*tell + 0.018*want\n",
      "Topic  35\n",
      "0.127*offic + 0.033*bottl + 0.029*lila + 0.021*session + 0.015*dragon + 0.015*smash + 0.013*dollar + 0.013*acquaint\n",
      "Topic  36\n",
      "0.076*tri + 0.051*run + 0.048*get + 0.025*chase + 0.018*away + 0.016*someon + 0.016*help + 0.015*scare\n",
      "Topic  37\n",
      "0.142*fli + 0.072*plane + 0.040*land + 0.034*travel + 0.028*air + 0.025*control + 0.024*flight + 0.018*crash\n",
      "Topic  38\n",
      "0.075*paint + 0.036*coffe + 0.029*art + 0.022*b + 0.022*refus + 0.018*sarah + 0.016*privat + 0.015*cup\n",
      "Topic  39\n",
      "0.067*river + 0.055*shoe + 0.043*won + 0.036*win + 0.036*pair + 0.026*bomb + 0.026*farm + 0.015*flirt\n",
      "Topic  40\n",
      "0.049*snow + 0.042*mirror + 0.041*boss + 0.034*unabl + 0.027*tower + 0.021*san + 0.019*awhil + 0.017*explor\n",
      "Topic  41\n",
      "0.176*play + 0.095*game + 0.039*ball + 0.039*team + 0.035*anim + 0.030*charact + 0.029*video + 0.020*watch\n",
      "Topic  42\n",
      "0.056*year + 0.051*mother + 0.036*father + 0.033*dream + 0.032*die + 0.027*old + 0.024*famili + 0.021*husband\n",
      "Topic  43\n",
      "0.130*new + 0.038*move + 0.032*citi + 0.031*space + 0.028*place + 0.025*flew + 0.020*york + 0.020*dish\n",
      "Topic  44\n",
      "0.147*class + 0.097*school + 0.063*teacher + 0.029*classroom + 0.020*teach + 0.015*kid + 0.015*desk + 0.013*deceas\n",
      "Topic  45\n",
      "0.033*went + 0.029*said + 0.025*go + 0.023*got + 0.018*came + 0.018*back + 0.016*just + 0.016*told\n",
      "Topic  46\n",
      "0.134*store + 0.128*dog + 0.083*shop + 0.043*church + 0.026*gift + 0.020*cake + 0.019*groceri + 0.019*buy\n",
      "Topic  47\n",
      "0.153*babi + 0.065*hospit + 0.048*doctor + 0.034*pregnant + 0.022*nurs + 0.021*care + 0.018*patient + 0.016*frank\n",
      "Topic  48\n",
      "0.178*bed + 0.072*sleep + 0.033*lie + 0.025*bedroom + 0.020*blanket + 0.018*sick + 0.015*grandfath + 0.014*room\n",
      "Topic  49\n",
      "0.040*flower + 0.038*dreamt + 0.037*bird + 0.033*god + 0.028*garden + 0.021*voic + 0.020*angel + 0.019*concert\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=50, num_words=8):\n",
    "    print \"Topic \", topic[0]\n",
    "    print topic[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \"491.  Wine high  Hard to remember how it started, over all the dream was searching for someone, or someplace, a lot took place in a grocery store. Everyone was stocking up, aed disaster was coming, people warn you to stay away from downtown. I take Emily up the coast to a high spot away from water. We follow the coast gliding our hands along the waves. At one point I end up paying 80 for a hotel but end up sleeping in a tent with 3 gay men. I get up to leave and go to the bar, where some guy I am dating is waiting for me. I explain to him that I have to leave, there is a man with a box telling me that I have a wish. I have a hard time thinking of what it should be.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(a)\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
